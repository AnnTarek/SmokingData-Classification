{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "X_train = train_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_train = train_df[['smoking']].to_numpy().reshape(-1) #Reshape from (n,1) to (n)\n",
    "\n",
    "X_val = val_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_val = val_df[['smoking']].to_numpy().reshape(-1)\n",
    "\n",
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df = train_df.copy()\n",
    "\n",
    "#Preprocessing\n",
    "normalized_df['hemoglobin'] = (normalized_df['hemoglobin'] - normalized_df['hemoglobin'].mean()) / normalized_df['hemoglobin'].std()\n",
    "normalized_df['hearing(right)'] = normalized_df['hearing(right)'] - 1\n",
    "normalized_df['fasting blood sugar'] = (normalized_df['fasting blood sugar'] - normalized_df['fasting blood sugar'].mean()) / normalized_df['fasting blood sugar'].std()\n",
    "normalized_df['LDL'] = (normalized_df['LDL'] - normalized_df['LDL'].mean()) / normalized_df['LDL'].std()\n",
    "normalized_df['height(cm)'] = (normalized_df['height(cm)'] - normalized_df['height(cm)'].mean()) / normalized_df['height(cm)'].std()\n",
    "normalized_df['weight(kg)'] = (normalized_df['weight(kg)'] - normalized_df['weight(kg)'].mean()) / normalized_df['weight(kg)'].std()\n",
    "normalized_df['Cholesterol'] = (normalized_df['Cholesterol'] - normalized_df['Cholesterol'].mean()) / normalized_df['Cholesterol'].std()\n",
    "normalized_df['serum creatinine'] = (normalized_df['serum creatinine'] - normalized_df['serum creatinine'].mean()) / normalized_df['serum creatinine'].std()\n",
    "normalized_df['Gtp'] = (normalized_df['Gtp'] - normalized_df['Gtp'].mean()) / normalized_df['Gtp'].std()\n",
    "\n",
    "X_train = normalized_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_train = normalized_df[['smoking']].to_numpy().reshape(-1) #Reshape from (n,1) to (n)\n",
    "num_samples, num_features = X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> ScikitLearn Adaboost </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7514206775297478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=100)\n",
    "adaboost.fit(X_train, y_train)\n",
    "predictions = adaboost.predict(X_train)\n",
    "score = adaboost.score(X_train, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Adaboost Implementation Using Decision Trees </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class MyAdaBoostTree:\n",
    "    def __init__(self, num_samples, num_features, num_iterations, max_tree_height = 1):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        self.num_iterations = num_iterations\n",
    "        self.max_tree_height = max_tree_height\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.sample_weights = np.ones((num_samples))/num_samples\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = DecisionTreeClassifier(criterion='gini', max_depth=self.max_tree_height)\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=self.sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, self.sample_weights).sum()  / self.sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            self.sample_weights = np.multiply(self.sample_weights, np.exp(2*alpha*incorrect))\n",
    "            self.sample_weights = self.sample_weights / self.sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas[idx] * np.where(prediction == 0, -1, prediction)        \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyAdaBoostLogistic:\n",
    "    def __init__(self, num_samples, num_features, num_iterations):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.sample_weights = np.ones((num_samples))/num_samples\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = LogisticRegression()\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=self.sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, self.sample_weights).sum()  / self.sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            self.sample_weights = np.multiply(self.sample_weights, np.exp(2*alpha*incorrect))\n",
    "            self.sample_weights = self.sample_weights / self.sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas[idx] * np.where(prediction == 0, -1, prediction)        \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/ X.shape[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7484851339047439\n"
     ]
    }
   ],
   "source": [
    "adaboost = MyAdaBoostTree(num_samples, num_features, 100)\n",
    "adaboost.train(X_train, y_train)\n",
    "print(adaboost.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7477081501946503\n"
     ]
    }
   ],
   "source": [
    "print(adaboost.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = MyAdaBoostLogistic(num_samples, num_features, 100)\n",
    "adaboost.train(X_train, y_train)\n",
    "print(adaboost.score(X_train, y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
