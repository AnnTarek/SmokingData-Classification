{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  127404\n",
      "Number of features:  10\n"
     ]
    }
   ],
   "source": [
    "#Reading Data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "X_train = train_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_train = train_df[['smoking']].to_numpy().reshape(-1) #Reshape from (n,1) to (n)\n",
    "\n",
    "X_val = val_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_val = val_df[['smoking']].to_numpy().reshape(-1)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"Number of samples: \", num_samples)\n",
    "print(\"Number of features: \", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MyAdaBoostTree(BaseEstimator):\n",
    "    def __init__(self, num_iterations=10, max_tree_height = 1):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.max_tree_height = max_tree_height\n",
    "\n",
    "    def train(self, X, y):\n",
    "        return self.fit(X,y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        self.alphas__ = []\n",
    "        self.models__ = []\n",
    "        sample_weights = np.ones((num_samples))/num_samples\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = DecisionTreeClassifier(criterion='gini', max_depth=self.max_tree_height)\n",
    "            weak_learner.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X)\n",
    "            incorrect = (sample_predictions != y)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, sample_weights).sum()  / sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas__.append(alpha)\n",
    "            self.models__.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            sample_weights = np.multiply(sample_weights, np.exp(2*alpha*incorrect))\n",
    "            sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models__):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas__[idx] * np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s      \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0] \n",
    "\n",
    "    def best_score_(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class MyAdaBoostLogistic:\n",
    "    def __init__(self, num_iterations=10):\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def train(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        self.alphas_ = []\n",
    "        self.models_ = []\n",
    "        sample_weights = np.ones((num_samples))/num_samples\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = LogisticRegression()\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, sample_weights).sum()  / sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas_.append(alpha)\n",
    "            self.models_.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            sample_weights = np.multiply(sample_weights, np.exp(2*alpha*incorrect))\n",
    "            sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models_):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas_[idx] * np.where(prediction == 0, -1, prediction)        \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/ X.shape[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from collections import Counter\n",
    "\n",
    "class MyRandomForest(BaseEstimator):\n",
    "    def __init__(self, num_trees=10, max_height=5, max_features=5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [] \n",
    "        num_samples = X.shape[0]       \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X[samples]\n",
    "            sampled_Y = y[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees_.append(tree)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees_])     \n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]   \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        return self.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7771890992433519\n",
      "0.7522918498053497\n"
     ]
    }
   ],
   "source": [
    "forest = MyRandomForest(num_trees=50, max_height=12, max_features=3)\n",
    "\n",
    "forest.train(X_train, y_train)\n",
    "\n",
    "print(forest.score(X_train, y_train))\n",
    "print(forest.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ann/miniconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791317384069574\n",
      "0.7533592867009921\n"
     ]
    }
   ],
   "source": [
    "## Bagging using sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=12, max_features=3), n_estimators=50)\n",
    "bagging.fit(X_train, y_train)\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing Bagging from scratch\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyBagging(BaseEstimator):\n",
    "    def __init__(self, num_trees=10, max_height=5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        num_samples  = X.shape[0]        \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X[samples]\n",
    "            sampled_Y = y[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    # calculate the prediction of each tree and return the maximum voted prediction\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        return self.fit(X, y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyBagging.__init__() got an unexpected keyword argument 'max_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Testing Bagging\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m bagging \u001b[38;5;241m=\u001b[39m \u001b[43mMyBagging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m bagging\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(bagging\u001b[38;5;241m.\u001b[39mscore(X_train, y_train))\n",
      "\u001b[0;31mTypeError\u001b[0m: MyBagging.__init__() got an unexpected keyword argument 'max_features'"
     ]
    }
   ],
   "source": [
    "## Testing Bagging\n",
    "\n",
    "bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging using KNN from scratch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class MyBaggingKNN(BaseEstimator):\n",
    "    def __init__(self, num_models=10, k=3):\n",
    "        self.num_models = num_models\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.models = []\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_models):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            model = KNeighborsClassifier(n_neighbors=self.k)\n",
    "            model.fit(sampled_X, sampled_Y)\n",
    "            self.models.append(model)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n",
    "    \n",
    "\n",
    "    def train(self, X, y):\n",
    "        return self.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834369407553923\n",
      "0.6906316714805978\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging KNN\n",
    "\n",
    "bagging = MyBaggingKNN(num_models=5, k=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Grid Search from scratch using threads\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class MyGridSearch():\n",
    "    def __init__(self, model, params, cv):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.cv = cv\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "        self.best_params = None\n",
    "        self.predictions = []\n",
    "\n",
    "    def worker(self,model, X_train, y_train, X_val, y_val):\n",
    "        print(f\"size of X_train: {X_train.shape}\")\n",
    "        print(f\"size of X_val: {X_val.shape}\")\n",
    "        model.train(X_train, y_train)\n",
    "        self.predictions.append((model.score(X_val, y_val), model.get_params()))\n",
    "\n",
    "    def grid_search(self):\n",
    "        parameters_grid = ParameterGrid(self.params)\n",
    "        number_of_models = len(parameters_grid)\n",
    "        print(f\"Number of models: {number_of_models}\")\n",
    "        for i in range(number_of_models):\n",
    "            # set the parameters for the model\n",
    "            self.model.set_params(**parameters_grid[i])\n",
    "            # Create a thread for each model\n",
    "            training_ratio = (self.cv - 1)  / self.cv\n",
    "            X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(X_train, y_train, train_size=training_ratio, random_state=42)\n",
    "            t = Thread(target=self.worker, args=(self.model, X_train_new, y_train_new, X_val_new, y_val_new))\n",
    "            t.start()\n",
    "            t.join()\n",
    "        return max(self.predictions)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models: 5\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "(0.7399238648404693, {'max_tree_height': 1, 'num_iterations': 50})\n"
     ]
    }
   ],
   "source": [
    "adaboost = MyAdaBoostTree(num_iterations= 100)\n",
    "clf = MyGridSearch(adaboost, {\"num_iterations\": [10, 20, 30, 40, 50]}, 5)\n",
    "print(clf.grid_search())\n",
    "\n",
    "#randomForest = MyRandomForest(num_trees=50, max_height=12, max_features=3, num_samples=num_samples)\n",
    "#clf = MyGridSearch(randomForest, {\"num_trees\": [10, 20, 30, 40, 50]}, 5)\n",
    "#print(clf.grid_search())\n",
    "\n",
    "#bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "#clf = MyGridSearch(bagging, {\"num_trees\": [10, 20, 30, 40, 50]}, 5)\n",
    "#print(clf.grid_search())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method MyGridSearch.best_params_ of <__main__.MyGridSearch object at 0x7efe83114280>>\n",
      "<bound method MyGridSearch.best_score_ of <__main__.MyGridSearch object at 0x7efe83114280>>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Grid Search for Random Forest\u001b[39;00m\n\u001b[1;32m     14\u001b[0m forest \u001b[38;5;241m=\u001b[39m MyRandomForest(num_trees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n",
      "Cell \u001b[0;32mIn[136], line 33\u001b[0m, in \u001b[0;36mMyGridSearch.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m(X, y)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Boosting\n",
    "adaboost = MyAdaBoostTree(num_samples, num_features)\n",
    "parameters = {'num_iterations': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "cv=5\n",
    "clf = MyGridSearch(adaboost, parameters, cv=cv)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "forest = MyRandomForest(num_trees=50, max_height=12, max_features=3, num_samples=num_samples)\n",
    "parameters = {'num_trees': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "clf = GridSearchCV(forest, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n",
    "\n",
    "# Grid Search for Bagging\n",
    "#bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "#parameters = {'num_trees': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "#clf = GridSearchCV(bagging, parameters, cv=5)\n",
    "#clf.fit(X_train, y_train)\n",
    "#print(clf.best_params_)\n",
    "#print(clf.best_score_)\n",
    "parameters = {'num_iterations': [10, 20, 30]}\n",
    "clf = GridSearchCV(adaboost, parameters, cv=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "random_forest = MyRandomForest()\n",
    "parameters = {'num_trees': [10, 20], 'max_height':[3, 5, 10], 'max_features':[2, 3, 5]}\n",
    "clf = GridSearchCV(random_forest, parameters, cv=2, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Bagging\n",
    "bagging = MyBagging()\n",
    "parameters = {'num_trees': [10, 20], 'max_height':[3, 5, 10]}\n",
    "clf = GridSearchCV(bagging, parameters, cv=2, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ..................................k=3, num_models=5; total time=  36.9s\n",
      "[CV] END ..................................k=3, num_models=5; total time=  41.1s\n",
      "[CV] END ..................................k=5, num_models=5; total time=  46.1s\n",
      "[CV] END ..................................k=5, num_models=5; total time=  50.1s\n",
      "[CV] END ..................................k=7, num_models=5; total time=  41.8s\n",
      "[CV] END ..................................k=7, num_models=5; total time=  44.6s\n",
      "{'k': 7, 'num_models': 5}\n",
      "0.7075523531443283\n",
      "0.7062664824814768\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for BaggingKNN\n",
    "bagging = MyBaggingKNN()\n",
    "parameters = {'num_models': [5], 'k':[3, 5, 7]}\n",
    "clf = GridSearchCV(bagging, parameters, cv=2, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Search from scratch\n",
    "from sklearn.model_selection import ParameterSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ...............max_tree_height=1, num_iterations=40; total time=   2.4s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=40; total time=   2.2s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=40; total time=   2.3s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=40; total time=   2.2s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=40; total time=   2.2s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.8s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.7s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.5s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.6s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.6s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.9s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.7s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.5s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.6s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.7s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=20; total time=   1.1s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=20; total time=   1.1s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=20; total time=   1.1s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=20; total time=   1.1s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=20; total time=   1.1s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.7s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.8s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.7s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.7s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=30; total time=   1.7s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=50; total time=   3.1s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=50; total time=   2.8s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=50; total time=   2.8s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=50; total time=   2.9s\n",
      "[CV] END ...............max_tree_height=1, num_iterations=50; total time=   2.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=30; total time=   2.9s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=30; total time=   3.0s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=30; total time=   2.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=30; total time=   2.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=30; total time=   2.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.9s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.6s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.7s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.7s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=50; total time=   4.6s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=40; total time=   3.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=40; total time=   3.7s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=40; total time=   3.6s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=40; total time=   3.7s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=40; total time=   3.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=20; total time=   1.9s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=20; total time=   1.9s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=20; total time=   1.8s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=20; total time=   1.9s\n",
      "[CV] END ...............max_tree_height=2, num_iterations=20; total time=   2.0s\n",
      "Mean cross-validated training accuracy score: 0.7468289932160491\n",
      "\n",
      "Best Estimator: MyAdaBoostTree(max_tree_height=2, num_iterations=50)\n",
      "\n",
      "Best Hyperparameter Combination: {'max_tree_height': 2, 'num_iterations': 50}\n",
      "\n",
      "Score: 0.7429988697726987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning using Random Search for AdaboostTree\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "hyperparameters = {'num_iterations': [10, 20, 30, 40, 50],\n",
    "                   'max_tree_height': randint(1, 3)}\n",
    "adaboost = MyAdaBoostTree()\n",
    "rs = RandomizedSearchCV(adaboost, hyperparameters, cv=5, verbose=2, random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "print(f\"Mean cross-validated training accuracy score: {rs.best_score_}\\n\")\n",
    "print(f\"Best Estimator: {rs.best_estimator_}\\n\")\n",
    "print(f\"Best Hyperparameter Combination: {rs.best_params_}\\n\")\n",
    "print(f\"Score: {rs.score(X_val, y_val)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning using Random Search for AdaboostLogistic\n",
    "\n",
    "####hyperparameters = {'num_iterations': [10, 20, 30, 40, 50]}\n",
    "###########until normalization occurs##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] END .........max_features=3, max_height=3, num_trees=50; total time=   2.6s\n",
      "[CV] END .........max_features=3, max_height=3, num_trees=50; total time=   2.5s\n",
      "[CV] END ........max_features=4, max_height=3, num_trees=200; total time=  10.3s\n",
      "[CV] END ........max_features=4, max_height=3, num_trees=200; total time=  10.1s\n",
      "[CV] END .........max_features=3, max_height=5, num_trees=50; total time=   3.1s\n",
      "[CV] END .........max_features=3, max_height=5, num_trees=50; total time=   3.1s\n",
      "[CV] END ........max_features=3, max_height=10, num_trees=50; total time=   4.5s\n",
      "[CV] END ........max_features=3, max_height=10, num_trees=50; total time=   4.6s\n",
      "[CV] END ........max_features=4, max_height=3, num_trees=100; total time=   5.2s\n",
      "[CV] END ........max_features=4, max_height=3, num_trees=100; total time=   5.4s\n",
      "[CV] END .......max_features=4, max_height=10, num_trees=500; total time=  52.2s\n",
      "[CV] END .......max_features=4, max_height=10, num_trees=500; total time=  51.7s\n",
      "[CV] END ........max_features=1, max_height=5, num_trees=100; total time=   3.9s\n",
      "[CV] END ........max_features=1, max_height=5, num_trees=100; total time=   3.9s\n",
      "[CV] END .........max_features=2, max_height=5, num_trees=20; total time=   1.3s\n",
      "[CV] END .........max_features=2, max_height=5, num_trees=20; total time=   1.2s\n",
      "[CV] END .........max_features=4, max_height=3, num_trees=10; total time=   0.8s\n",
      "[CV] END .........max_features=4, max_height=3, num_trees=10; total time=   0.8s\n",
      "[CV] END ........max_features=4, max_height=5, num_trees=500; total time=  32.7s\n",
      "[CV] END ........max_features=4, max_height=5, num_trees=500; total time=  32.7s\n",
      "Mean cross-validated training accuracy score: 0.7527942607767417\n",
      "\n",
      "Best Estimator: MyRandomForest(max_features=4, max_height=10, num_trees=500)\n",
      "\n",
      "Best Hyperparameter Combination: {'max_features': 4, 'max_height': 10, 'num_trees': 500}\n",
      "\n",
      "Score: 0.7507220896646992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning using Random Search for Random Forest\n",
    "hyperparameters = {'num_trees': [10, 20, 50, 100, 200, 500],\n",
    "                    'max_height':[3, 5, 10], \n",
    "                    'max_features': randint(1, 5)}\n",
    "forest = MyRandomForest()\n",
    "rs = RandomizedSearchCV(forest, hyperparameters, cv=2, verbose=2, random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "print(f\"Mean cross-validated training accuracy score: {rs.best_score_}\\n\")\n",
    "print(f\"Best Estimator: {rs.best_estimator_}\\n\")\n",
    "print(f\"Best Hyperparameter Combination: {rs.best_params_}\\n\")\n",
    "print(f\"Score: {rs.score(X_val, y_val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] END ........................max_height=5, num_trees=200; total time=  26.0s\n",
      "[CV] END ........................max_height=5, num_trees=200; total time=  24.7s\n",
      "[CV] END ........................max_height=10, num_trees=20; total time=   4.8s\n",
      "[CV] END ........................max_height=10, num_trees=20; total time=   4.7s\n",
      "[CV] END .........................max_height=3, num_trees=10; total time=   1.2s\n",
      "[CV] END .........................max_height=3, num_trees=10; total time=   1.2s\n",
      "[CV] END .......................max_height=10, num_trees=100; total time=  22.1s\n",
      "[CV] END .......................max_height=10, num_trees=100; total time=  21.5s\n",
      "[CV] END .........................max_height=5, num_trees=10; total time=   1.5s\n",
      "[CV] END .........................max_height=5, num_trees=10; total time=   1.6s\n",
      "[CV] END ........................max_height=5, num_trees=100; total time=  12.2s\n",
      "[CV] END ........................max_height=5, num_trees=100; total time=  12.8s\n",
      "[CV] END .........................max_height=3, num_trees=50; total time=   4.8s\n",
      "[CV] END .........................max_height=3, num_trees=50; total time=   4.9s\n",
      "[CV] END .........................max_height=3, num_trees=20; total time=   2.3s\n",
      "[CV] END .........................max_height=3, num_trees=20; total time=   2.2s\n",
      "[CV] END .......................max_height=10, num_trees=200; total time=  43.9s\n",
      "[CV] END .......................max_height=10, num_trees=200; total time=  43.1s\n",
      "[CV] END ........................max_height=3, num_trees=200; total time=  17.5s\n",
      "[CV] END ........................max_height=3, num_trees=200; total time=  17.0s\n",
      "Mean cross-validated training accuracy score: 0.7511852061159775\n",
      "\n",
      "Best Estimator: MyBagging(max_height=10, num_trees=200)\n",
      "\n",
      "Best Hyperparameter Combination: {'num_trees': 200, 'max_height': 10}\n",
      "\n",
      "Score: 0.7513499937209595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning using Random Search for bagging\n",
    "hyperparameters = {'num_trees': [10, 20, 50, 100, 200],\n",
    "                    'max_height':[3, 5, 10]}\n",
    "bagging = MyBagging()\n",
    "rs = RandomizedSearchCV(bagging, hyperparameters, cv=2, verbose=2, random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "print(f\"Mean cross-validated training accuracy score: {rs.best_score_}\\n\")\n",
    "print(f\"Best Estimator: {rs.best_estimator_}\\n\")\n",
    "print(f\"Best Hyperparameter Combination: {rs.best_params_}\\n\")\n",
    "print(f\"Score: {rs.score(X_val, y_val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ann/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 3 is smaller than n_iter=10. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ..................................k=3, num_models=5; total time=  29.6s\n",
      "[CV] END ..................................k=3, num_models=5; total time=  28.3s\n",
      "[CV] END ..................................k=5, num_models=5; total time=  29.7s\n",
      "[CV] END ..................................k=5, num_models=5; total time=  28.4s\n",
      "[CV] END ..................................k=7, num_models=5; total time=  29.9s\n",
      "[CV] END ..................................k=7, num_models=5; total time=  30.5s\n",
      "Mean cross-validated training accuracy score: 0.7071363536466673\n",
      "\n",
      "Best Estimator: MyBaggingKNN(k=7, num_models=5)\n",
      "\n",
      "Best Hyperparameter Combination: {'num_models': 5, 'k': 7}\n",
      "\n",
      "Score: 0.7048223031520784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning using Random Search for baggingKNN\n",
    "hyperparameters = {'num_models': [5],\n",
    "                    'k': [3,5,7]}\n",
    "baggingKNN = MyBaggingKNN()\n",
    "rs = RandomizedSearchCV(baggingKNN, hyperparameters, cv=2, verbose=2, random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "print(f\"Mean cross-validated training accuracy score: {rs.best_score_}\\n\")\n",
    "print(f\"Best Estimator: {rs.best_estimator_}\\n\")\n",
    "print(f\"Best Hyperparameter Combination: {rs.best_params_}\\n\")\n",
    "print(f\"Score: {rs.score(X_val, y_val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
