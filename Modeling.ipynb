{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  127404\n",
      "Number of features:  10\n"
     ]
    }
   ],
   "source": [
    "#Reading Data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "X_train = train_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_train = train_df[['smoking']].to_numpy().reshape(-1) #Reshape from (n,1) to (n)\n",
    "\n",
    "X_val = val_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_val = val_df[['smoking']].to_numpy().reshape(-1)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"Number of samples: \", num_samples)\n",
    "print(\"Number of features: \", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MyAdaBoostTree(BaseEstimator):\n",
    "    def __init__(self, num_iterations=10, max_tree_height = 1):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.max_tree_height = max_tree_height\n",
    "\n",
    "    def train(self, X, y):\n",
    "        return self.fit(X,y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        self.alphas__ = []\n",
    "        self.models__ = []\n",
    "        sample_weights = np.ones((num_samples))/num_samples\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = DecisionTreeClassifier(criterion='gini', max_depth=self.max_tree_height)\n",
    "            weak_learner.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X)\n",
    "            incorrect = (sample_predictions != y)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, sample_weights).sum()  / sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas__.append(alpha)\n",
    "            self.models__.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            sample_weights = np.multiply(sample_weights, np.exp(2*alpha*incorrect))\n",
    "            sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models__):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas__[idx] * np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s      \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0] \n",
    "\n",
    "    def best_score_(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class MyAdaBoostLogistic:\n",
    "    def __init__(self, num_iterations=10):\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def train(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        self.alphas_ = []\n",
    "        self.models_ = []\n",
    "        sample_weights = np.ones((num_samples))/num_samples\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = LogisticRegression()\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, sample_weights).sum()  / sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas_.append(alpha)\n",
    "            self.models_.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            sample_weights = np.multiply(sample_weights, np.exp(2*alpha*incorrect))\n",
    "            sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models_):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas_[idx] * np.where(prediction == 0, -1, prediction)        \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/ X.shape[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MyRandomForest():\n",
    "    def __init__(self, num_trees, max_height, max_features, num_samples):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def train(self, X_train, y_train):        \n",
    "        for i in range(self.num_trees):\n",
    "            print(f\"num_samples: {self.num_samples}\")\n",
    "            samples = np.random.choice(self.num_samples, size=self.num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            prediction = tree.predict(X)\n",
    "            sum_predictions +=  np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s         \n",
    "        return np.where(sum_predictions > 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]   \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_trees\": self.num_trees, \"max_height\": self.max_height, \"max_features\": self.max_features, \"num_samples\": self.num_samples}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        return self\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7778562682490345\n",
      "0.7522918498053497\n"
     ]
    }
   ],
   "source": [
    "forest = MyRandomForest(num_trees=50, max_height=12, max_features=3)\n",
    "\n",
    "forest.train(X_train, y_train)\n",
    "\n",
    "print(forest.score(X_train, y_train))\n",
    "print(forest.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maram/.local/lib/python3.10/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7913566293052023\n",
      "0.7524174306166018\n"
     ]
    }
   ],
   "source": [
    "## Bagging using sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=12, max_features=3), n_estimators=50)\n",
    "bagging.fit(X_train, y_train)\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing Bagging from scratch\n",
    "from collections import Counter\n",
    "\n",
    "class MyBagging():\n",
    "    def __init__(self, num_trees, max_height, max_features):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    # calculate the prediction of each tree and return the maximum voted prediction\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n",
    "    \n",
    "    def fit(self, X, y,cv):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_trees\": self.num_trees, \"max_height\": self.max_height, \"max_features\": self.max_features, \"num_features\": self.max_features}\n",
    "    \n",
    "    def set_params(self, **parameters): \n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7744419327493642\n",
      "0.7505965088534472\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging\n",
    "\n",
    "bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging using KNN from scratch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class MyBaggingKNN():\n",
    "    def __init__(self, num_models, k):\n",
    "        self.num_models = num_models\n",
    "        self.k = k\n",
    "        self.models = []\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_models):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            model = KNeighborsClassifier(n_neighbors=self.k)\n",
    "            model.fit(sampled_X, sampled_Y)\n",
    "            self.models.append(model)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834000502339016\n",
      "0.6904433002637197\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging KNN\n",
    "\n",
    "bagging = MyBaggingKNN(num_models=5, k=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Grid Search from scratch using threads\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class MyGridSearch():\n",
    "    def __init__(self, model, params, cv):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.cv = cv\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "        self.best_params = None\n",
    "        self.predictions = []\n",
    "\n",
    "    def worker(self,model, X_train, y_train, X_val, y_val):\n",
    "        print(f\"size of X_train: {X_train.shape}\")\n",
    "        print(f\"size of X_val: {X_val.shape}\")\n",
    "        model.train(X_train, y_train)\n",
    "        self.predictions.append((model.score(X_val, y_val), model.get_params()))\n",
    "\n",
    "    def grid_search(self):\n",
    "        parameters_grid = ParameterGrid(self.params)\n",
    "        number_of_models = len(parameters_grid)\n",
    "        print(f\"Number of models: {number_of_models}\")\n",
    "        for i in range(number_of_models):\n",
    "            # set the parameters for the model\n",
    "            self.model.set_params(**parameters_grid[i])\n",
    "            # Create a thread for each model\n",
    "            training_ratio = (self.cv - 1)  / self.cv\n",
    "            X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(X_train, y_train, train_size=training_ratio, random_state=42)\n",
    "            t = Thread(target=self.worker, args=(self.model, X_train_new, y_train_new, X_val_new, y_val_new))\n",
    "            t.start()\n",
    "            t.join()\n",
    "        return max(self.predictions)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models: 5\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "size of X_train: (101923, 10)\n",
      "size of X_val: (25481, 10)\n",
      "(0.7399238648404693, {'max_tree_height': 1, 'num_iterations': 50})\n"
     ]
    }
   ],
   "source": [
    "adaboost = MyAdaBoostTree(num_iterations= 100)\n",
    "clf = MyGridSearch(adaboost, {\"num_iterations\": [10, 20, 30, 40, 50]}, 5)\n",
    "print(clf.grid_search())\n",
    "\n",
    "#randomForest = MyRandomForest(num_trees=50, max_height=12, max_features=3, num_samples=num_samples)\n",
    "#clf = MyGridSearch(randomForest, {\"num_trees\": [10, 20, 30, 40, 50]}, 5)\n",
    "#print(clf.grid_search())\n",
    "\n",
    "#bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "#clf = MyGridSearch(bagging, {\"num_trees\": [10, 20, 30, 40, 50]}, 5)\n",
    "#print(clf.grid_search())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method MyGridSearch.best_params_ of <__main__.MyGridSearch object at 0x7efe83114280>>\n",
      "<bound method MyGridSearch.best_score_ of <__main__.MyGridSearch object at 0x7efe83114280>>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Grid Search for Random Forest\u001b[39;00m\n\u001b[1;32m     14\u001b[0m forest \u001b[38;5;241m=\u001b[39m MyRandomForest(num_trees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)\n",
      "Cell \u001b[0;32mIn[136], line 33\u001b[0m, in \u001b[0;36mMyGridSearch.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m(X, y)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Boosting\n",
    "adaboost = MyAdaBoostTree(num_samples, num_features)\n",
    "parameters = {'num_iterations': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "cv=5\n",
    "clf = MyGridSearch(adaboost, parameters, cv=cv)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "forest = MyRandomForest(num_trees=50, max_height=12, max_features=3, num_samples=num_samples)\n",
    "parameters = {'num_trees': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "clf = GridSearchCV(forest, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n",
    "\n",
    "# Grid Search for Bagging\n",
    "#bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "#parameters = {'num_trees': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "#clf = GridSearchCV(bagging, parameters, cv=5)\n",
    "#clf.fit(X_train, y_train)\n",
    "#print(clf.best_params_)\n",
    "#print(clf.best_score_)\n",
    "parameters = {'num_iterations': [10, 20, 30]}\n",
    "clf = GridSearchCV(adaboost, parameters, cv=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
