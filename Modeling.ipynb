{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  127404\n",
      "Number of features:  10\n"
     ]
    }
   ],
   "source": [
    "#Reading Data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "X_train = train_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_train = train_df[['smoking']].to_numpy().reshape(-1) #Reshape from (n,1) to (n)\n",
    "\n",
    "X_val = val_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_val = val_df[['smoking']].to_numpy().reshape(-1)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"Number of samples: \", num_samples)\n",
    "print(\"Number of features: \", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "class MyAdaBoostTree(AdaBoostClassifier):\n",
    "    def __init__(self, num_samples, num_features, num_iterations, max_tree_height = 1):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        self.num_iterations = num_iterations\n",
    "        self.max_tree_height = max_tree_height\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.sample_weights = np.ones((num_samples))/num_samples\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = DecisionTreeClassifier(criterion='gini', max_depth=self.max_tree_height)\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=self.sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, self.sample_weights).sum()  / self.sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            self.sample_weights = np.multiply(self.sample_weights, np.exp(2*alpha*incorrect))\n",
    "            self.sample_weights = self.sample_weights / self.sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas[idx] * np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s      \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]   \n",
    "\n",
    "\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        print(f\"num_features: {self.num_features}, num_samples: {self.num_samples}, num_iterations: {self.num_iterations}, max_tree_height: {self.max_tree_height}, sample_weights: {self.sample_weights}\")\n",
    "        return {\"num_iterations\": self.num_iterations, \"max_tree_height\": self.max_tree_height, \"num_samples\": self.num_samples, \"num_features\": self.num_features}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            print(f\"Setting {parameter} to {value}\")\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class MyAdaBoostLogistic:\n",
    "    def __init__(self, num_samples, num_features, num_iterations):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.sample_weights = np.ones((num_samples))/num_samples\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = LogisticRegression()\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=self.sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, self.sample_weights).sum()  / self.sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            self.sample_weights = np.multiply(self.sample_weights, np.exp(2*alpha*incorrect))\n",
    "            self.sample_weights = self.sample_weights / self.sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas[idx] * np.where(prediction == 0, -1, prediction)        \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/ X.shape[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = MyAdaBoostTree(num_samples, num_features, 100)\n",
    "adaboost.train(X_train, y_train)\n",
    "print(adaboost.score(X_val, y_val))\n",
    "\n",
    "adaboost = MyAdaBoostLogistic(num_samples, num_features, 100)\n",
    "adaboost.train(X_train, y_train)\n",
    "print(adaboost.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MyRandomForest():\n",
    "    def __init__(self, num_trees, max_height, max_features, num_samples):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def train(self, X_train, y_train):        \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(self.num_samples, size=self.num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            prediction = tree.predict(X)\n",
    "            sum_predictions +=  np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s         \n",
    "        return np.where(sum_predictions > 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]   \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_trees\": self.num_trees, \"max_height\": self.max_height, \"max_features\": self.max_features, \"num_samples\": self.num_samples}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7778562682490345\n",
      "0.7522918498053497\n"
     ]
    }
   ],
   "source": [
    "forest = MyRandomForest(num_trees=50, max_height=12, max_features=3)\n",
    "\n",
    "forest.train(X_train, y_train)\n",
    "\n",
    "print(forest.score(X_train, y_train))\n",
    "print(forest.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maram/.local/lib/python3.10/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7913566293052023\n",
      "0.7524174306166018\n"
     ]
    }
   ],
   "source": [
    "## Bagging using sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=12, max_features=3), n_estimators=50)\n",
    "bagging.fit(X_train, y_train)\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing Bagging from scratch\n",
    "from collections import Counter\n",
    "\n",
    "class MyBagging():\n",
    "    def __init__(self, num_trees, max_height, max_features):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    # calculate the prediction of each tree and return the maximum voted prediction\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_trees\": self.num_trees, \"max_height\": self.max_height, \"max_features\": self.max_features, \"num_features\": self.max_features, \"num_samples\": self.num_samples}\n",
    "    \n",
    "    def set_params(self, **parameters): \n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7744419327493642\n",
      "0.7505965088534472\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging\n",
    "\n",
    "bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging using KNN from scratch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class MyBaggingKNN():\n",
    "    def __init__(self, num_models, k):\n",
    "        self.num_models = num_models\n",
    "        self.k = k\n",
    "        self.models = []\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_models):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            model = KNeighborsClassifier(n_neighbors=self.k)\n",
    "            model.fit(sampled_X, sampled_Y)\n",
    "            self.models.append(model)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834000502339016\n",
      "0.6904433002637197\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging KNN\n",
    "\n",
    "bagging = MyBaggingKNN(num_models=5, k=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 10\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 10\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 10\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 10\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 10\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 20\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 20\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 20\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 20\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 20\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 50\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 50\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 50\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 50\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 50\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 100\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 100\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 100\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 100\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 100\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 200\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 200\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 200\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 200\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 200\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 300\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 300\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 300\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 300\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 300\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 400\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 400\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 400\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 400\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 400\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 500\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 500\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 500\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 500\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "num_features: 10, num_samples: 127404, num_iterations: 100, max_tree_height: 1, sample_weights: [7.84904713e-06 7.84904713e-06 7.84904713e-06 ... 7.84904713e-06\n",
      " 7.84904713e-06 7.84904713e-06]\n",
      "Setting num_iterations to 500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 40 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/maram/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/tmp/ipykernel_14048/969004525.py\", line 45, in fit\n    self.train(X, y, sample_weight=self.sample_weights)\nTypeError: MyAdaBoostTree.train() got an unexpected keyword argument 'sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m500\u001b[39m]}\n\u001b[1;32m      7\u001b[0m clf \u001b[38;5;241m=\u001b[39m GridSearchCV(adaboost, parameters, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 875\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 40 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/maram/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/tmp/ipykernel_14048/969004525.py\", line 45, in fit\n    self.train(X, y, sample_weight=self.sample_weights)\nTypeError: MyAdaBoostTree.train() got an unexpected keyword argument 'sample_weight'\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Boosting\n",
    "adaboost = MyAdaBoostTree(num_samples, num_features, 100)\n",
    "parameters = {'num_iterations': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "clf = GridSearchCV(adaboost, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "#forest = MyRandomForest(num_trees=50, max_height=12, max_features=3, num_samples=num_samples)\n",
    "#parameters = {'num_trees': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "#clf = GridSearchCV(forest, parameters, cv=5)\n",
    "#clf.fit(X_train, y_train)\n",
    "#print(clf.best_params_)\n",
    "#print(clf.best_score_)\n",
    "#print(clf.score(X_val, y_val))\n",
    "\n",
    "# Grid Search for Bagging\n",
    "#bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "#parameters = {'num_trees': [10, 20, 50, 100, 200, 300, 400, 500]}\n",
    "#clf = GridSearchCV(bagging, parameters, cv=5)\n",
    "#clf.fit(X_train, y_train)\n",
    "#print(clf.best_params_)\n",
    "#print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
