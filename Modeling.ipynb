{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  127404\n",
      "Number of features:  10\n"
     ]
    }
   ],
   "source": [
    "#Reading Data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "X_train = train_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_train = train_df[['smoking']].to_numpy().reshape(-1) #Reshape from (n,1) to (n)\n",
    "\n",
    "X_val = val_df.drop(columns=['Unnamed: 0', 'smoking']).to_numpy()\n",
    "y_val = val_df[['smoking']].to_numpy().reshape(-1)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"Number of samples: \", num_samples)\n",
    "print(\"Number of features: \", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MyAdaBoostTree(BaseEstimator):\n",
    "    def __init__(self, num_iterations=10, max_tree_height = 1):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.max_tree_height = max_tree_height\n",
    "\n",
    "    def train(self, X, y):\n",
    "        return self.fit(X,y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        self.alphas__ = []\n",
    "        self.models__ = []\n",
    "        sample_weights = np.ones((num_samples))/num_samples\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = DecisionTreeClassifier(criterion='gini', max_depth=self.max_tree_height)\n",
    "            weak_learner.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X)\n",
    "            incorrect = (sample_predictions != y)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, sample_weights).sum()  / sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas__.append(alpha)\n",
    "            self.models__.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            sample_weights = np.multiply(sample_weights, np.exp(2*alpha*incorrect))\n",
    "            sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models__):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas__[idx] * np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s      \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0] \n",
    "\n",
    "\n",
    "\n",
    "class MyAdaBoostLogistic:\n",
    "    def __init__(self, num_iterations=10):\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def train(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        self.alphas_ = []\n",
    "        self.models_ = []\n",
    "        sample_weights = np.ones((num_samples))/num_samples\n",
    "        for iteration in range(self.num_iterations):\n",
    "            weak_learner = LogisticRegression()\n",
    "            weak_learner.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            sample_predictions = weak_learner.predict(X_train)\n",
    "            incorrect = (sample_predictions != y_train)*1 #Multiply by 1 to convert True/False to 1/0\n",
    "            weighted_error = np.multiply(incorrect, sample_weights).sum()  / sample_weights.sum()\n",
    "            alpha = (0.5) * math.log((1-weighted_error) / weighted_error)\n",
    "            \n",
    "            #Add Model and Alpha to Ensemble\n",
    "            self.alphas_.append(alpha)\n",
    "            self.models_.append(weak_learner)\n",
    "            \n",
    "            #Update Weights\n",
    "            sample_weights = np.multiply(sample_weights, np.exp(2*alpha*incorrect))\n",
    "            sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for idx, model in enumerate(self.models_):\n",
    "            prediction = model.predict(X)\n",
    "            sum_predictions += self.alphas_[idx] * np.where(prediction == 0, -1, prediction)        \n",
    "        return np.where(sum_predictions >= 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/ X.shape[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyRandomForest(BaseEstimator):\n",
    "    def __init__(self, num_trees=10, max_height=5, max_features=5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [] \n",
    "        num_samples = X.shape[0]       \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X[samples]\n",
    "            sampled_Y = y[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees_.append(tree)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        sum_predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.trees_:\n",
    "            prediction = tree.predict(X)\n",
    "            sum_predictions +=  np.where(prediction == 0, -1, prediction)  #np.where used to replace 0s with -1s         \n",
    "        return np.where(sum_predictions > 0, 1, 0)\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]   \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        return self.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7775737025525101\n",
      "0.7529197538616099\n"
     ]
    }
   ],
   "source": [
    "forest = MyRandomForest(num_trees=50, max_height=12, max_features=3)\n",
    "\n",
    "forest.train(X_train, y_train)\n",
    "\n",
    "print(forest.score(X_train, y_train))\n",
    "print(forest.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maram/.local/lib/python3.10/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7913566293052023\n",
      "0.7524174306166018\n"
     ]
    }
   ],
   "source": [
    "## Bagging using sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=12, max_features=3), n_estimators=50)\n",
    "bagging.fit(X_train, y_train)\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing Bagging from scratch\n",
    "from collections import Counter\n",
    "\n",
    "class MyBagging():\n",
    "    def __init__(self, num_trees, max_height, max_features):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_trees):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_height, max_features=self.max_features)\n",
    "            tree.fit(sampled_X, sampled_Y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    # calculate the prediction of each tree and return the maximum voted prediction\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.train(X, y)\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_trees\": self.num_trees, \"max_height\": self.max_height, \"max_features\": self.max_features, \"num_features\": self.max_features, \"num_samples\": self.num_samples}\n",
    "    \n",
    "    def set_params(self, **parameters): \n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7744419327493642\n",
      "0.7505965088534472\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging\n",
    "\n",
    "bagging = MyBagging(num_trees=10, max_height=12, max_features=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging using KNN from scratch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class MyBaggingKNN():\n",
    "    def __init__(self, num_models, k):\n",
    "        self.num_models = num_models\n",
    "        self.k = k\n",
    "        self.models = []\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        num_samples  = X_train.shape[0]        \n",
    "        for i in range(self.num_models):\n",
    "            samples = np.random.choice(num_samples, size=num_samples, replace=True)\n",
    "            sampled_X = X_train[samples]\n",
    "            sampled_Y = y_train[samples]\n",
    "            model = KNeighborsClassifier(n_neighbors=self.k)\n",
    "            model.fit(sampled_X, sampled_Y)\n",
    "            self.models.append(model)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        mode_predictions =np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "        return mode_predictions\n",
    "    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        prediction = self.predict(X)\n",
    "        return (prediction == y).sum()/  X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834000502339016\n",
      "0.6904433002637197\n"
     ]
    }
   ],
   "source": [
    "## Testing Bagging KNN\n",
    "\n",
    "bagging = MyBaggingKNN(num_models=5, k=3)\n",
    "bagging.train(X_train, y_train)\n",
    "\n",
    "print(bagging.score(X_train, y_train))\n",
    "print(bagging.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Boosting\n",
    "adaboost = MyAdaBoostTree()\n",
    "parameters = {'num_iterations': [10, 20, 30]}\n",
    "clf = GridSearchCV(adaboost, parameters, cv=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "random_forest = MyRandomForest()\n",
    "parameters = {'num_trees': [10, 20], 'max_height':[3, 5, 10], 'max_features':[2, 3, 5]}\n",
    "clf = GridSearchCV(random_forest, parameters, cv=2, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(X_val, y_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
